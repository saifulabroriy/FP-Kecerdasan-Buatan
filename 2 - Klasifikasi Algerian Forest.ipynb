{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a4b742",
   "metadata": {},
   "source": [
    "# FP-Kecerdasan-Buatan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6755fa",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "737c2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafea280",
   "metadata": {},
   "source": [
    "## Load Data Training\n",
    "\n",
    "### Attribute Information:\n",
    "1. Date : (DD/MM/YYYY) Day, month ('june' to 'september'), year (2012) Weather data observations\n",
    "2. Temp : temperature noon (temperature max) in Celsius degrees: 22 to 42\n",
    "3. RH : Relative Humidity in %: 21 to 90\n",
    "4. Ws :Wind speed in km/h: 6 to 29\n",
    "5. Rain: total day in mm: 0 to 16.8 FWI Components\n",
    "6. Fine Fuel Moisture Code (FFMC) index from the FWI system: 28.6 to 92.5\n",
    "7. Duff Moisture Code (DMC) index from the FWI system: 1.1 to 65.9\n",
    "8. Drought Code (DC) index from the FWI system: 7 to 220.4\n",
    "9. Initial Spread Index (ISI) index from the FWI system: 0 to 18.5\n",
    "10. Buildup Index (BUI) index from the FWI system: 1.1 to 68\n",
    "11. Fire Weather Index (FWI) Index: 0 to 31.1\n",
    "12. Classes: two classes, namely fire and not fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea661c3d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Temperature</th>\n",
       "      <th>RH</th>\n",
       "      <th>Ws</th>\n",
       "      <th>Rain</th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>BUI</th>\n",
       "      <th>FWI</th>\n",
       "      <th>Classes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.7</td>\n",
       "      <td>3.4</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>not fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>61</td>\n",
       "      <td>13</td>\n",
       "      <td>1.3</td>\n",
       "      <td>64.4</td>\n",
       "      <td>4.1</td>\n",
       "      <td>7.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>not fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>13.1</td>\n",
       "      <td>47.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>not fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>2.5</td>\n",
       "      <td>28.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>not fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>77</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>not fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Temperature   RH   Ws  Rain   FFMC  DMC    DC  ISI  BUI  FWI      Classes\n",
       "0           29   57   18    0.0  65.7  3.4   7.6  1.3  3.4  0.5  not fire   \n",
       "1           29   61   13    1.3  64.4  4.1   7.6  1.0  3.9  0.4  not fire   \n",
       "2           26   82   22   13.1  47.1  2.5   7.1  0.3  2.7  0.1  not fire   \n",
       "3           25   89   13    2.5  28.6  1.3   6.9  0.0  1.7  0.0  not fire   \n",
       "4           27   77   16    0.0  64.8  3.0  14.2  1.2  3.9  0.5  not fire   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read fie csv\n",
    "forest_df = pd.read_csv('./dataset/training.csv', header=[0])\n",
    "\n",
    "# Drop kolom hari, bulan, dan tahun karena tidak dibutuhkan\n",
    "clean_df = forest_df.drop(columns=[\"day\", \"month\", \"year\"])\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedbd3d",
   "metadata": {},
   "source": [
    "## Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df0cc009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29.  57.  18.  ...  1.3  3.4  0.5]\n",
      " [29.  61.  13.  ...  1.   3.9  0.4]\n",
      " [26.  82.  22.  ...  0.3  2.7  0.1]\n",
      " ...\n",
      " [27.  87.  29.  ...  0.4  3.4  0.2]\n",
      " [24.  54.  18.  ...  1.7  5.1  0.7]\n",
      " [24.  64.  15.  ...  1.2  4.8  0.5]]\n"
     ]
    }
   ],
   "source": [
    "batch_string = np.array(clean_df.drop(columns=['Classes']))\n",
    "\n",
    "batch = []\n",
    "for sample in batch_string:\n",
    "    _ = []\n",
    "    for attribute in sample:\n",
    "        _.append(float(attribute))\n",
    "    batch.append(_)\n",
    "\n",
    "batch = np.array(batch)\n",
    "\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0d8aad",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcedc52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "classes = np.array(clean_df['Classes'])\n",
    "\n",
    "outputs = []\n",
    "for item in classes:\n",
    "    if(item.strip() == 'not fire'):\n",
    "        outputs.append(0)\n",
    "    elif(item.strip() == 'fire'):\n",
    "        outputs.append(1)\n",
    "\n",
    "y = np.array(outputs)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87544a05",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04b0803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Class\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Generate random weights\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons) / 100\n",
    "        \n",
    "        # Make array fill of zeros\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Keep input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # dot product of input with weight plus bias\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Get derivative value of weight and bias\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Get input derivative\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# Activation Function ReLU Class\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Keep input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # ReLU function\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # copy dvalues\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # if inputs <= 0 then make it 0\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Activation Softmax Class\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Eksponensial dan Prevent Overflow\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        # Normalisasi\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "        # produce probabilites distribution\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(- 1, 1)\n",
    "            \n",
    "            # Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "\n",
    "# Loss Class\n",
    "class Loss:\n",
    "    # Calculate loss\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        \n",
    "        # Mean of losses\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Categorical Cross Entropy Class\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip to prevent -log(0) because it produces INF(infinite)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # Number of labels in every sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = - y_true / dvalues\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy ():\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        \n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        \n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Sthocastic Gradient Descent\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += - self.learning_rate * layer.dweights\n",
    "        layer.biases += - self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de14cfc",
   "metadata": {},
   "source": [
    "## Training NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f5524c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.9631147540983607, loss: 0.6920619987753959\n",
      "epoch: 100, acc: 1.0, loss: 0.0021185921409403135\n",
      "epoch: 200, acc: 1.0, loss: 0.000718985059005452\n",
      "epoch: 300, acc: 1.0, loss: 0.00040894981697428974\n",
      "epoch: 400, acc: 1.0, loss: 0.0002790384272991017\n",
      "epoch: 500, acc: 1.0, loss: 0.00020905074544308634\n",
      "epoch: 600, acc: 1.0, loss: 0.00016578966518063863\n",
      "epoch: 700, acc: 1.0, loss: 0.00013661246641968417\n",
      "epoch: 800, acc: 1.0, loss: 0.00011570921075354717\n",
      "epoch: 900, acc: 1.0, loss: 0.00010005494137938699\n",
      "epoch: 1000, acc: 1.0, loss: 8.792701810608727e-05\n",
      "epoch: 1100, acc: 1.0, loss: 7.827560588258657e-05\n",
      "epoch: 1200, acc: 1.0, loss: 7.042628863398778e-05\n",
      "epoch: 1300, acc: 1.0, loss: 6.392690493701519e-05\n",
      "epoch: 1400, acc: 1.0, loss: 5.8463459422726613e-05\n",
      "epoch: 1500, acc: 1.0, loss: 5.3811342496364776e-05\n",
      "epoch: 1600, acc: 1.0, loss: 4.980587105283189e-05\n",
      "epoch: 1700, acc: 1.0, loss: 4.632360571704212e-05\n",
      "epoch: 1800, acc: 1.0, loss: 4.327036479869193e-05\n",
      "epoch: 1900, acc: 1.0, loss: 4.0573054168165944e-05\n",
      "epoch: 2000, acc: 1.0, loss: 3.81741152353017e-05\n",
      "epoch: 2100, acc: 1.0, loss: 3.602763845869051e-05\n",
      "epoch: 2200, acc: 1.0, loss: 3.40966036638177e-05\n",
      "epoch: 2300, acc: 1.0, loss: 3.235077324874765e-05\n",
      "epoch: 2400, acc: 1.0, loss: 3.07653240717566e-05\n",
      "epoch: 2500, acc: 1.0, loss: 2.93195495859159e-05\n",
      "epoch: 2600, acc: 1.0, loss: 2.7996158786844653e-05\n",
      "epoch: 2700, acc: 1.0, loss: 2.678055950827842e-05\n",
      "epoch: 2800, acc: 1.0, loss: 2.5660402882462202e-05\n",
      "epoch: 2900, acc: 1.0, loss: 2.4625063983970864e-05\n",
      "epoch: 3000, acc: 1.0, loss: 2.3665474090766544e-05\n",
      "epoch: 3100, acc: 1.0, loss: 2.2773784139911213e-05\n",
      "epoch: 3200, acc: 1.0, loss: 2.194321088086101e-05\n",
      "epoch: 3300, acc: 1.0, loss: 2.1167785014662364e-05\n",
      "epoch: 3400, acc: 1.0, loss: 2.0442330061658918e-05\n",
      "epoch: 3500, acc: 1.0, loss: 1.9762294776067706e-05\n",
      "epoch: 3600, acc: 1.0, loss: 1.9123587448622448e-05\n",
      "epoch: 3700, acc: 1.0, loss: 1.852262261201231e-05\n",
      "epoch: 3800, acc: 1.0, loss: 1.795623052338985e-05\n",
      "epoch: 3900, acc: 1.0, loss: 1.7421569277807624e-05\n",
      "epoch: 4000, acc: 1.0, loss: 1.6916095090875876e-05\n",
      "epoch: 4100, acc: 1.0, loss: 1.64375316503606e-05\n",
      "epoch: 4200, acc: 1.0, loss: 1.59838637672672e-05\n",
      "epoch: 4300, acc: 1.0, loss: 1.5553231891706712e-05\n",
      "epoch: 4400, acc: 1.0, loss: 1.5143947374247502e-05\n",
      "epoch: 4500, acc: 1.0, loss: 1.47544864516136e-05\n",
      "epoch: 4600, acc: 1.0, loss: 1.438347292184373e-05\n",
      "epoch: 4700, acc: 1.0, loss: 1.4029653597150806e-05\n",
      "epoch: 4800, acc: 1.0, loss: 1.3691882723708178e-05\n",
      "epoch: 4900, acc: 1.0, loss: 1.3369116547917849e-05\n",
      "epoch: 5000, acc: 1.0, loss: 1.3060398356344258e-05\n",
      "epoch: 5100, acc: 1.0, loss: 1.2764849530678355e-05\n",
      "epoch: 5200, acc: 1.0, loss: 1.248166240467798e-05\n",
      "epoch: 5300, acc: 1.0, loss: 1.2210093312691107e-05\n",
      "epoch: 5400, acc: 1.0, loss: 1.1949456416276384e-05\n",
      "epoch: 5500, acc: 1.0, loss: 1.1699118228782612e-05\n",
      "epoch: 5600, acc: 1.0, loss: 1.1458502908387138e-05\n",
      "epoch: 5700, acc: 1.0, loss: 1.1227058305571667e-05\n",
      "epoch: 5800, acc: 1.0, loss: 1.1004287557224902e-05\n",
      "epoch: 5900, acc: 1.0, loss: 1.0789728633419403e-05\n",
      "epoch: 6000, acc: 1.0, loss: 1.0582932755068333e-05\n",
      "epoch: 6100, acc: 1.0, loss: 1.0383493777857636e-05\n",
      "epoch: 6200, acc: 1.0, loss: 1.0191034028279417e-05\n",
      "epoch: 6300, acc: 1.0, loss: 1.0005200960857511e-05\n",
      "epoch: 6400, acc: 1.0, loss: 9.825665144711756e-06\n",
      "epoch: 6500, acc: 1.0, loss: 9.652118380660718e-06\n",
      "epoch: 6600, acc: 1.0, loss: 9.484272040457946e-06\n",
      "epoch: 6700, acc: 1.0, loss: 9.32185554798963e-06\n",
      "epoch: 6800, acc: 1.0, loss: 9.16461499912398e-06\n",
      "epoch: 6900, acc: 1.0, loss: 9.012311905808596e-06\n",
      "epoch: 7000, acc: 1.0, loss: 8.864722051450193e-06\n",
      "epoch: 7100, acc: 1.0, loss: 8.721634446154536e-06\n",
      "epoch: 7200, acc: 1.0, loss: 8.582849871882691e-06\n",
      "epoch: 7300, acc: 1.0, loss: 8.448181523504394e-06\n",
      "epoch: 7400, acc: 1.0, loss: 8.31746011745762e-06\n",
      "epoch: 7500, acc: 1.0, loss: 8.190512640390756e-06\n",
      "epoch: 7600, acc: 1.0, loss: 8.067188389311444e-06\n",
      "epoch: 7700, acc: 1.0, loss: 7.947329698592145e-06\n",
      "epoch: 7800, acc: 1.0, loss: 7.830795327400246e-06\n",
      "epoch: 7900, acc: 1.0, loss: 7.717456361757233e-06\n",
      "epoch: 8000, acc: 1.0, loss: 7.607184908494998e-06\n",
      "epoch: 8100, acc: 1.0, loss: 7.499857141614097e-06\n",
      "epoch: 8200, acc: 1.0, loss: 7.395359127662319e-06\n",
      "epoch: 8300, acc: 1.0, loss: 7.2935826733663215e-06\n",
      "epoch: 8400, acc: 1.0, loss: 7.194424975037014e-06\n",
      "epoch: 8500, acc: 1.0, loss: 7.097788286234183e-06\n",
      "epoch: 8600, acc: 1.0, loss: 7.003579654257616e-06\n",
      "epoch: 8700, acc: 1.0, loss: 6.911710533937319e-06\n",
      "epoch: 8800, acc: 1.0, loss: 6.822096581590917e-06\n",
      "epoch: 8900, acc: 1.0, loss: 6.734657438737801e-06\n",
      "epoch: 9000, acc: 1.0, loss: 6.649316473188208e-06\n",
      "epoch: 9100, acc: 1.0, loss: 6.566000582822733e-06\n",
      "epoch: 9200, acc: 1.0, loss: 6.4846399631908975e-06\n",
      "epoch: 9300, acc: 1.0, loss: 6.405167896961338e-06\n",
      "epoch: 9400, acc: 1.0, loss: 6.327520741508887e-06\n",
      "epoch: 9500, acc: 1.0, loss: 6.251637663078707e-06\n",
      "epoch: 9600, acc: 1.0, loss: 6.177460425355487e-06\n",
      "epoch: 9700, acc: 1.0, loss: 6.104933293451292e-06\n",
      "epoch: 9800, acc: 1.0, loss: 6.034014930351816e-06\n",
      "epoch: 9900, acc: 1.0, loss: 5.9646429444851106e-06\n",
      "epoch: 10000, acc: 1.0, loss: 5.896767322198085e-06\n"
     ]
    }
   ],
   "source": [
    "# Hidden Layer 1\n",
    "dense1 = Layer_Dense(10, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Hidden Layer 2\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# Output Layer\n",
    "dense3 = Layer_Dense(64, 2)\n",
    "\n",
    "# loss and softmax activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=0.01)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10001):\n",
    "    dense1.forward(batch)\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    dense3.forward(activation2.output)\n",
    "\n",
    "    loss = loss_activation.forward(dense3.output, y)\n",
    "\n",
    "    preds = np.argmax(loss_activation.output, axis=1)\n",
    "\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(preds == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, acc: {accuracy}, loss: {loss}')\n",
    "        \n",
    "    # Backpropagation\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense3.backward(loss_activation.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c1c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f942bd98faa300fc7578274a659425881f1887d9c61941f7b7b8701bdb2de32e"
  },
  "kernelspec": {
   "display_name": "fp_kecerdasan_buatan",
   "language": "python",
   "name": "fp_kecerdasan_buatan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
